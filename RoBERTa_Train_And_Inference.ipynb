{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of braindead transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxqbihn10I7I"
      },
      "source": [
        "# **TRAIN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38M7dQZ7nk_5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iiDLQoPSTIK"
      },
      "source": [
        "!pip install -U git+https://github.com/Adapter-Hub/adapter-transformers.git "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4rEkNpMSdrz"
      },
      "source": [
        "import json\n",
        "import torch\n",
        "import sklearn\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers import RobertaConfig, RobertaModelWithHeads\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
        "from transformers import TextClassificationPipeline\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEflGQGt5g4z"
      },
      "source": [
        "## LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEmUkiYzSiGM"
      },
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "train_path = '/content/drive/MyDrive/Machine Learning/datasets/how2/train.jsonl' #CHANGE gdrive to drive and vice versa if it can't find the dataset\n",
        "test_path = '/content/drive/MyDrive/Machine Learning/datasets/how2/test.jsonl'\n",
        "dev_path = '/content/drive/MyDrive/Machine Learning/datasets/how2/valid.jsonl'\n",
        "dataset_name = 'amazon' # CHANGE\n",
        "\n",
        "def read_split(split_dir, dataset):  #dataset is the folder containing the dataset'ag', 'chemprot' etc.\n",
        "    file_path = Path(split_dir.format(dataset))\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        for id_, row in enumerate(f):\n",
        "            data = json.loads(row)\n",
        "            texts.append(data[\"text\"]) #change\n",
        "            labels.append(data[\"label\"])\n",
        "            \n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = read_split(train_path, dataset_name)\n",
        "test_texts, test_labels = read_split(test_path,dataset_name)\n",
        "val_texts, val_labels = read_split(dev_path, dataset_name)\n",
        "try:\n",
        "  classes = [x.item() for x in np.unique(train_labels)]\n",
        "except:\n",
        "  pass\n",
        "datapoints = {'Train': len(train_texts), 'Test': len(test_texts), 'valid': len(val_texts)}\n",
        "print('NUM DATAPOINTS:\\n', datapoints)\n",
        "print('\\nNUM CLASSES: ', len(classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omv-BUzY5Qqa"
      },
      "source": [
        "## PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0FMXJcn5Hy1"
      },
      "source": [
        "#REMOVE UNWANTED CLASSES\n",
        "class_list = []\n",
        "class_list = set(class_list)\n",
        "temp_labels = list()\n",
        "temp_texts = list()\n",
        "for x in range(len(train_labels)):\n",
        "  if (not (train_labels[x] in class_list)):\n",
        "    temp_labels.append(train_labels[x])\n",
        "    temp_texts.append(train_texts[x])\n",
        "train_texts = temp_texts\n",
        "train_labels = temp_labels\n",
        "\n",
        "temp_labels = list()\n",
        "temp_texts = list()\n",
        "for x in range(len(val_labels)):\n",
        "  if (not (val_labels[x] in class_list)):\n",
        "    temp_labels.append(val_labels[x])\n",
        "    temp_texts.append(val_texts[x])\n",
        "val_texts = temp_texts\n",
        "val_labels = temp_labels\n",
        "\n",
        "temp_labels = list()\n",
        "temp_texts = list()\n",
        "for x in range(len(test_labels)):\n",
        "  if (not (test_labels[x] in class_list)):\n",
        "    temp_labels.append(test_labels[x])\n",
        "    temp_texts.append(test_texts[x])\n",
        "test_texts = temp_texts\n",
        "test_labels = temp_labels\n",
        "\n",
        "try:\n",
        "  classes = [x.item() for x in np.unique(train_labels)]\n",
        "except:\n",
        "  pass\n",
        "datapoints = {'Train': len(train_texts), 'Test': len(test_texts), 'valid': len(val_texts)}\n",
        "print('NUM DATAPOINTS:\\n', datapoints)\n",
        "print('\\nNUM CLASSES: ', len(classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUuIHufgCsye"
      },
      "source": [
        "#REMOVE STOP WORDS and normalize case\n",
        "def remove_stop_words(data):\n",
        "  words = set(stopwords.words(\"english\"))\n",
        "  return [' '.join([word for word in text.split() if word not in words]).lower() for text in data]\n",
        "\n",
        "train_texts = remove_stop_words(train_texts)\n",
        "test_texts = remove_stop_words(test_texts)\n",
        "val_texts = remove_stop_words(val_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYZEsmMHJq9D"
      },
      "source": [
        "## TRAIN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNbQgOfiSo-9"
      },
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9GUof-YSrQK"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, max_length=65, truncation=True, padding=\"max_length\")\n",
        "val_encodings = tokenizer(val_texts, max_length=65, truncation=True, padding=\"max_length\")\n",
        "test_encodings = tokenizer(test_texts, max_length=65, truncation=True, padding=\"max_length\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a84o0hEAStOq"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset): # might need to change depending on label type\n",
        "    def __init__(self, encodings, labels, classes):\n",
        "        self.encodings = encodings\n",
        "        class_dict = {val: key for key, val in enumerate(classes)} \n",
        "        self.labels = [class_dict[x] for x in labels]\n",
        "        self.classes = classes\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def len_classes(self):\n",
        "      return len(self.classes)\n",
        "\n",
        "train_dataset = Dataset(train_encodings, train_labels, classes)\n",
        "val_dataset = Dataset(val_encodings, val_labels, classes)\n",
        "test_dataset = Dataset(test_encodings, test_labels, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T06zidjbSyZg"
      },
      "source": [
        "config = RobertaConfig.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    num_labels=train_dataset.len_classes(),\n",
        ")\n",
        "model = RobertaModelWithHeads.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    config=config,\n",
        ")\n",
        "# Add a new adapter\n",
        "adapter_name = \"Name\"\n",
        "model.add_adapter(adapter_name)\n",
        "id2label = {id: label for (id, label) in enumerate(train_dataset.classes)} #MAY HAVE TO CHANGE DEPENDING ON LABELS OF THE DATASET\n",
        "# Add a matching classification head\n",
        "model.add_classification_head(\n",
        "    adapter_name,\n",
        "    num_labels=train_dataset.len_classes(),\n",
        "    id2label=id2label #{ 0: \"üëé\", 1: \"üëç\"} \n",
        "  )\n",
        "# Activate the adapter\n",
        "model.train_adapter(adapter_name)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=1e-4,  #CHANGE\n",
        "    num_train_epochs=75,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    logging_steps=500,              \n",
        "    output_dir=\"./training_output\",\n",
        "    overwrite_output_dir=True,\n",
        "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "def compute_accuracy(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {\"acc\": (preds == p.label_ids).mean()}\n",
        "\n",
        "def compute_f1_macro(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {'f1 macro':sklearn.metrics.f1_score(p.label_ids, preds, average='macro')}\n",
        "\n",
        "def compute_f1_micro(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {'f1 micro':sklearn.metrics.f1_score(p.label_ids, preds, average='micro')}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_f1_macro,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3FyvhD3S1Rd"
      },
      "source": [
        "trainer.train()\n",
        "eval = trainer.evaluate()\n",
        "print(eval)\n",
        "model.save_adapter('/content/drive/MyDrive/Machine Learning/TextClassification/oct/', adapter_name)\n",
        "with open('/content/drive/MyDrive/Machine Learning/TextClassification/oct/EVALS.json', 'w') as f:\n",
        "  json.dump(eval, f, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9T91NOd08sH"
      },
      "source": [
        "# **RUN SAVED MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTAEq_cO05Vi"
      },
      "source": [
        "!pip install -U git+https://github.com/Adapter-Hub/adapter-transformers.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGKNhxE2kpb8"
      },
      "source": [
        "import json\n",
        "import torch\n",
        "import sklearn\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers import RobertaConfig, RobertaModelWithHeads\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
        "from transformers import TextClassificationPipeline\n",
        "from transformers import AdapterLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZevF4s4gulXy"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgqip7mHkxtL"
      },
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "config = RobertaConfig.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    num_labels=len(classes),\n",
        ")\n",
        "model = RobertaModelWithHeads.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "weightsdir = '/content/drive/MyDrive/Machine Learning/TextClassification/oct/'\n",
        "\n",
        "x = model.load_adapter(weightsdir)\n",
        "model.set_active_adapters(x)\n",
        "\n",
        "classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjQsnrFFlYTD"
      },
      "source": [
        "c = classifier('Example')\n",
        "print('Label: ',c[0]['label'], 'score: ', c[0]['score'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}